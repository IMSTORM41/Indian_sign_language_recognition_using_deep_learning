{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"attention model final.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"63b440ff"},"source":["# importing all libraries"],"id":"63b440ff"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rf1tCu0PjsXY","executionInfo":{"status":"ok","timestamp":1624779865179,"user_tz":-330,"elapsed":24561,"user":{"displayName":"Deepak Singh","photoUrl":"","userId":"00401075196702607176"}},"outputId":"34dce83e-a47e-4785-b2ab-cba7fc53eb30"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"Rf1tCu0PjsXY","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"18aa7dd6"},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, TimeDistributed, LSTM, Input, BatchNormalization, Conv2D, MaxPooling2D, Reshape, Conv1D, GlobalAveragePooling1D, MaxPooling1D, Lambda\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.losses import categorical_crossentropy\n","import tensorflow_hub as hub\n","from PIL import Image\n","import gzip\n","from nltk.translate.bleu_score import corpus_bleu\n","from keras.preprocessing.sequence import pad_sequences\n","import spacy\n","import h5py\n","import os\n","import cv2\n","import pickle\n","import re\n","import shutil\n","import glob\n","import gzip\n","%matplotlib inline"],"id":"18aa7dd6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"311d2bbb"},"source":["import time"],"id":"311d2bbb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27ada957"},"source":["# creating path variables"],"id":"27ada957"},{"cell_type":"code","metadata":{"id":"a31a4269"},"source":["path_gloss_df = '/content/drive/MyDrive/Colab Notebooks/gloss_vector_dataframe.zip'\n","path_embedding_images = '/content/drive/MyDrive/Colab Notebooks/emb_comp_i3d_train_zero.h5'\n","path_gloss_files = '/content/drive/MyDrive/Colab Notebooks/phoenix14t.pami0.train.annotations_only.gzip'"],"id":"a31a4269","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"da7c1127"},"source":["path_dev_gloss = '/content/drive/MyDrive/Colab Notebooks/phoenix14t.pami0.dev.annotations_only.gzip'\n","path_test_gloss = '/content/drive/MyDrive/Colab Notebooks/phoenix14t.pami0.test.annotations_only.gzip'"],"id":"da7c1127","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73c53be6"},"source":["path_embedding_dev = '/content/drive/MyDrive/Colab Notebooks/emb_comp_i3d_dev_zero.h5'\n","path_embedding_test = '/content/drive/MyDrive/Colab Notebooks/emb_comp_i3d_test_zero.h5'"],"id":"73c53be6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26877bba"},"source":["dev_img_embedding = \"\\\\emb_comp_i3d_dev_zero.h5\"\n","test_img_embedding = \"\\\\emb_comp_i3d_test_zero.h5\""],"id":"26877bba","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8fac48b"},"source":["# loading annotations in a dataframes"],"id":"f8fac48b"},{"cell_type":"code","metadata":{"id":"d5574411"},"source":["with gzip.open(path_gloss_files,'rb') as f:\n","  annotation_gloss = pickle.load(f)"],"id":"d5574411","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a03a69a"},"source":["with gzip.open(path_dev_gloss,'rb') as f:\n","  annotation_dev = pickle.load(f)"],"id":"9a03a69a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bd2716c4"},"source":["with gzip.open(path_test_gloss,'rb')as f :\n","  annotation_test = pickle.load(f)"],"id":"bd2716c4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08cb2d14"},"source":["annotation_gloss = pd.DataFrame(annotation_gloss)"],"id":"08cb2d14","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e084379c"},"source":["annotation_gloss_dev = pd.DataFrame(annotation_dev)\n","\n"],"id":"e084379c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0560922"},"source":["annotation_gloss_test = pd.DataFrame(annotation_test)"],"id":"d0560922","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10d0a7e5"},"source":["annotation_gloss.head()"],"id":"10d0a7e5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2f98a65"},"source":["annotation_gloss_dev.head()"],"id":"e2f98a65","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8f8fd671"},"source":["annotation_gloss_test.head()"],"id":"8f8fd671","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3974480d"},"source":["# we can use values attribute to directly create a list of all sentences of a particular column\n","#annotation_gloss_test[\"text\"].values"],"id":"3974480d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ac23abda"},"source":["# preprocessing the vocabulary"],"id":"ac23abda"},{"cell_type":"code","metadata":{"id":"eb97a3c0"},"source":["for i in range(len(annotation_gloss)):\n","  annotation_gloss.iloc[i,2] = 'startseq '+annotation_gloss.iloc[i,2].lower() + ' endseq'\n","  annotation_gloss.iloc[i,3] = 'startseq '+annotation_gloss.iloc[i,3].lower() + ' endseq'\n","  annotation_gloss.iloc[i,3] = annotation_gloss.iloc[i,3].replace(' .','')\n","  #annotation_gloss.iloc[i,3] =annotation_gloss.iloc[i,3][-2].replace(' ','')\n"," "],"id":"eb97a3c0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94dc95aa"},"source":["for i in range(len(annotation_gloss_dev)):\n","  annotation_gloss_dev.iloc[i,2] = 'startseq '+annotation_gloss_dev.iloc[i,2].lower() + ' endseq'\n","  annotation_gloss_dev.iloc[i,3] = 'startseq '+annotation_gloss_dev.iloc[i,3].lower() + ' endseq'\n","  annotation_gloss_dev.iloc[i,3] = annotation_gloss_dev.iloc[i,3].replace(' .','')"],"id":"94dc95aa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08b295a6"},"source":["for i in range(len(annotation_gloss_test)):\n","  annotation_gloss_test.iloc[i,2] = 'startseq '+annotation_gloss_test.iloc[i,2].lower() + ' endseq'\n","  annotation_gloss_test.iloc[i,3] = 'startseq '+annotation_gloss_test.iloc[i,3].lower() + ' endseq'\n","  annotation_gloss_test.iloc[i,3] = annotation_gloss_test.iloc[i,3].replace(' .','')"],"id":"08b295a6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba43a367"},"source":["# creating a vocabulary of texts"],"id":"ba43a367"},{"cell_type":"code","metadata":{"id":"d9f97a41"},"source":["vocabulary = []\n","for txt in annotation_gloss.text.values:\n","    vocabulary.extend(txt.split())\n","for txt in annotation_gloss.gloss.values:\n","    vocabulary.extend(txt.split())\n","    \n","for txt in annotation_gloss_dev.text.values:\n","    vocabulary.extend(txt.split())\n","for txt in annotation_gloss_dev.gloss.values:\n","    vocabulary.extend(txt.split())\n","    \n","for txt in annotation_gloss_test.text.values:\n","    vocabulary.extend(txt.split())\n","for txt in annotation_gloss_test.gloss.values:\n","    vocabulary.extend(txt.split())\n","print('Vocabulary Size: %d' % len(set(vocabulary)))"],"id":"d9f97a41","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81613bb0"},"source":["# reading images embedding for train test and dev"],"id":"81613bb0"},{"cell_type":"code","metadata":{"id":"95b659c7"},"source":["#reading images feature vectors\n","with h5py.File(path_embedding_images,'r')as ab:\n","    ind = ab.keys()\n","    print(ind)\n","    data_train=ab['x_1'][()]"],"id":"95b659c7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a04b9b36"},"source":["#reading images feature vectors\n","with h5py.File(path_embedding_dev,'r')as ab:\n","    ind = ab.keys()\n","    print(ind)\n","    data_dev=ab['x_1'][()]"],"id":"a04b9b36","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"183435a3"},"source":["#reading images feature vectors\n","with h5py.File(path_embedding_test,'r')as ab:\n","    ind = ab.keys()\n","    print(ind)\n","    data_test=ab['x_1'][()]"],"id":"183435a3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"54a9af91"},"source":["data_train.shape"],"id":"54a9af91","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"826c5786"},"source":["data_dev.shape"],"id":"826c5786","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1833bbfc"},"source":["data_test.shape"],"id":"1833bbfc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2d4ec29"},"source":["# creating tokenizer dataset\n"],"id":"e2d4ec29"},{"cell_type":"code","metadata":{"id":"2e641fde"},"source":["gloss_token = []\n","text_token = []\n","for i in range(len(annotation_gloss)):\n","  gloss_token.append(annotation_gloss.iloc[i,2])\n","  text_token.append(annotation_gloss.iloc[i,3])"],"id":"2e641fde","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a50242b"},"source":["gloss_token_dev = []\n","text_token_dev = []\n","for i in range(len(annotation_gloss_dev)):\n","  gloss_token_dev.append(annotation_gloss_dev.iloc[i,2])\n","  text_token_dev.append(annotation_gloss_dev.iloc[i,3])"],"id":"8a50242b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"297bdd83"},"source":["gloss_token_test = []\n","text_token_test = []\n","for i in range(len(annotation_gloss_test)):\n","  gloss_token_test.append(annotation_gloss_test.iloc[i,2])\n","  text_token_test.append(annotation_gloss_test.iloc[i,3])"],"id":"297bdd83","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8640cdab"},"source":[""],"id":"8640cdab","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bb21ae9f"},"source":["# creating tokenizer"],"id":"bb21ae9f"},{"cell_type":"code","metadata":{"id":"66f032ae"},"source":["def create_tokenizer(vocabulary):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=3606,\n","                                                 oov_token=\"<unk>\",\n","                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","    tokenizer.fit_on_texts(vocabulary)\n","    tokenizer.word_index['<pad>'] = 0\n","    tokenizer.index_word[0] = '<pad>'\n","    return tokenizer"],"id":"66f032ae","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ea7a06f6"},"source":["tokenizer = create_tokenizer(vocabulary)"],"id":"ea7a06f6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5e4295c7"},"source":["vocab_size = len(tokenizer.word_index)+1\n","print(\"vocalb size\",vocab_size)"],"id":"5e4295c7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bffc4ada"},"source":["tokenizer.index_word[3]"],"id":"bffc4ada","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b3e47ac"},"source":[""],"id":"9b3e47ac","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eef9ac8f"},"source":["def encode_sequences(tokenizer, length, lines):\n","    # integer encode sequences\n","    X = tokenizer.texts_to_sequences(lines)\n","    # pad sequences with 0 values\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    return X"],"id":"eef9ac8f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cc3493d7"},"source":["# tokenizing train,test,dev glosses"],"id":"cc3493d7"},{"cell_type":"code","metadata":{"id":"703955fc"},"source":["train_y = encode_sequences(tokenizer,35,gloss_token)\n","dev_y = encode_sequences(tokenizer,35,gloss_token_dev)\n","test_y = encode_sequences(tokenizer,35,gloss_token_test)"],"id":"703955fc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a98115a6"},"source":["train_y[0]"],"id":"a98115a6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97c700aa"},"source":[""],"id":"97c700aa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1bdab79"},"source":[""],"id":"d1bdab79","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0b38796d"},"source":["# hyperparameters"],"id":"0b38796d"},{"cell_type":"code","metadata":{"id":"8ae834ab"},"source":["BATCH_SIZE = 128\n","BUFFER_SIZE = 7096\n","embedding_dim = 256\n","units = 512\n","vocab_size = len(tokenizer.word_index) + 1\n","num_steps = 7096 // BATCH_SIZE\n","features_shape = 512\n","attention_features_shape = 35"],"id":"8ae834ab","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-eoUYx4_1AV5"},"source":["data_train.shape"],"id":"-eoUYx4_1AV5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQraY8hq1GRP"},"source":["# for i in train_y[2]:\n","#   print(tokenizer.index_word[i])"],"id":"hQraY8hq1GRP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73b9b1b3"},"source":["dataset = tf.data.Dataset.from_tensor_slices((data_train, train_y))\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# dataset shape = ((128,512),(128,35))\n","# dataset shape = ((batchsize,feature size),(batch size ,sentence size))"],"id":"73b9b1b3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59e1aac4"},"source":["for (batch, (img_tensor, target)) in enumerate(dataset):\n","    print(\"batch\",batch)\n","    \n","    print(\"img_tensor\",img_tensor)\n","    print(\"img tensrog shape\",img_tensor.shape)\n","    print(\"target\",target)\n","    print(\"tartghet shape\",target.shape)\n","    break"],"id":"59e1aac4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"745102f0"},"source":["# creating model for decoder"],"id":"745102f0"},{"cell_type":"code","metadata":{"id":"26ca0099"},"source":["'''The encoder output(i.e. 'features'), hidden state(initialized to 0)(i.e. 'hidden') and\n","the decoder input (which is the start token)(i.e. 'x') is passed to the decoder.'''\n","\n","class Rnn_Local_Decoder(tf.keras.Model):\n","    def __init__(self, embedding_dim, units, vocab_size):\n","        super(Rnn_Local_Decoder, self).__init__()\n","        self.units = units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(self.units,\n","                                         activation='tanh',\n","                                         recurrent_activation='sigmoid',\n","                                      use_bias=True,\n","                                      kernel_initializer='glorot_uniform',\n","                                      return_sequences=True,\n","                                      return_state=True,\n","                                      recurrent_initializer='glorot_uniform')\n","\n","        self.fc1 = tf.keras.layers.Dense(self.units)\n","\n","        self.dropout = tf.keras.layers.Dropout(0.9, noise_shape=None, seed=None)\n","        self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n","\n","        self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","        # Implementing Attention Mechanism\n","        self.Uattn = tf.keras.layers.Dense(units)\n","        self.Wattn = tf.keras.layers.Dense(units)\n","        self.Vattn = tf.keras.layers.Dense(1)\n","\n","    def call(self, x, features, hidden):\n","        print(\"x\",x.shape)\n","        #x shape is (128,1)\n","        b_shape = features.shape[0]\n","        \n","        # features shape ==> (128,512) ==> Output from ENCODER\n","        # hidden shape == (batch_size, hidden_size) ==>(128,512)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (128,1,512)\n","        \n","        \n","        \n","        print(\"hidden shape\",hidden.shape)\n","        \n","        #hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        \n","        \n","        #print(\"hidden shape\",hidden_with_time_axis.shape)\n","\n","        # score shape == (128, 512, 1)\n","        # Attention Function\n","        '''e(ij) = f(s(t-1),h(j))'''\n","        ''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n","        \n","        #print(\"self ua tten \",self.Uattn(features).shape)\n","        #print(\" wattn shape\",self.Wattn(hidden_with_time_axis))\n","        \n","        #temp_tanh = tf.reshape(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden)),[128,512,1])\n","        #print(\"tanh_temp sahpe \",temp_tanh.shape)\n","        #print(\"tanh sahpe \",tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden)).shape)\n","\n","\n","\n","\n","\n","        #score = self.Vattn(tf.reshape(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden)),(BATCH_SIZE,512,1)))\n","        score = self.Vattn(tf.reshape(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden)),(b_shape,512,1)))\n","\n","\n","\n","\n","        print(\"score shape\",score.shape)\n","        \n","        \n","\n","        # self.Uattn(features) : (128,512)\n","        # self.Wattn(hidden_with_time_axis) : (128,1,512)\n","        # tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (128,512,1)\n","        # self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (128,512,1) ==> score\n","\n","        # you get 1 at the last axis because you are applying score to self.Vattn\n","        # Then find Probability using Softmax\n","        '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        temp_attention = tf.cast(attention_weights,dtype=tf.float64)\n","        #print(\"attention shapes \",temp_attention.shape)\n","\n","        # attention_weights shape == (128,128, 1)\n","        # Give weights to the different pixels in the image\n","        ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n","        #print(\"attention type \",type(attention_weights))\n","        #print(\"attention weithg datatype\",temp_attention.dtype)\n","        #print(\"features type\",type(features))\n","        #print(\"feature dtype\",features.dtype)\n","        print(\"attention shape \",temp_attention.shape)\n","        \n","        \n","        temp_features = tf.expand_dims(features,1)\n","        print(\"feature shape\",temp_features.shape)\n","\n","        context_vector = temp_attention * temp_features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        print(\"context vector shape\",context_vector.shape)\n","\n","        # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n","        # context_vector shape after sum == (64, 256)\n","        # x shape after passing through embedding == (64, 1, 256)\n","\n","        x = self.embedding(x)\n","        # x shape after concatenation == (64, 1,  512)\n","        temp_x = tf.cast(x,dtype=tf.float64)\n","\n","        x = tf.concat([tf.expand_dims(context_vector, 1), temp_x], axis=-1)\n","        # passing the concatenated vector to the GRU\n","\n","        output, state = self.lstm(x)\n","        # shape == (batch_size, max_length, hidden_size)\n","\n","        x = self.fc1(output)\n","        # x shape == (batch_size * max_length, hidden_size)\n","\n","        x = tf.reshape(x, (-1, x.shape[2]))\n","\n","        # Adding Dropout and BatchNorm Layers\n","        x= self.dropout(x)\n","        x= self.batchnormalization(x)\n","\n","        # output shape == (64 * 512)\n","        x = self.fc2(x)\n","\n","        # shape : (64 * 8329(vocab))\n","        return x, state, attention_weights\n","\n","    def reset_state(self, batch_size):\n","        return tf.zeros((batch_size, self.units))\n","    # def build_graph(self):\n","    #     x = Input(shape=(128,512))\n","    #     return Model(inputs=[x], outputs=self.call(x))\n","\n","\n","\n","decoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)\n","# decoder.build((features,hidden))\n","# decoder.build_graph().summary()"],"id":"26ca0099","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HKuS7uXFYcm"},"source":["# tf.keras.utils.plot_model(\n","#     decoder.build_graph(),                      # here is the trick (for now)\n","#     to_file='\\content\\drive\\MyDrive\\Colab Notebooks\\model.png', dpi=96,              # saving  \n","#     show_shapes=True, show_layer_names=True,  # show shapes and layer name\n","#     expand_nested=False                       # will show nested block\n","# )"],"id":"3HKuS7uXFYcm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v9yKryEFYR2"},"source":[""],"id":"-v9yKryEFYR2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8baa7c2a"},"source":[" # loss"],"id":"8baa7c2a"},{"cell_type":"code","metadata":{"id":"efad4a2a"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","   from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"id":"efad4a2a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3e04c435"},"source":["# training"],"id":"3e04c435"},{"cell_type":"code","metadata":{"id":"06268c0f"},"source":["temp = tf.expand_dims([tokenizer.word_index['startseq']] * BATCH_SIZE, 1)"],"id":"06268c0f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70254533"},"source":["temp.shape"],"id":"70254533","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"54f77c16"},"source":["loss_plot = []\n","\n","@tf.function\n","def train_step(img_tensor, target):\n"," loss = 0\n"," # initializing the hidden state for each batch\n"," # because the captions are not related from image to image\n","\n"," # hidden shape = (128,1024)\n"," hidden = decoder.reset_state(batch_size=target.shape[0])\n"," dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n"," \n","\n"," with tf.GradientTape() as tape:\n","     features = img_tensor\n","     for i in range(1, target.shape[1]):\n","         # passing the features through the decoder\n","         predictions, hidden, _ = decoder(dec_input, features, hidden)\n","        #  decoder.build((features,hidden))\n","        #  decoder.build_graph().summary()\n","        #  tf.keras.utils.plot_model(\n","        #           decoder.build_graph(),                      # here is the trick (for now)\n","        #           to_file='\\content\\drive\\MyDrive\\Colab Notebooks\\model.png', dpi=96,              # saving  \n","        #           show_shapes=True, show_layer_names=True,  # show shapes and layer name\n","        #           expand_nested=False                       # will show nested block\n","        #             )\n","         \n","         #print(\"predicted value\",predictions)\n","         loss += loss_function(target[:, i], predictions)\n","\n","         # using teacher forcing\n","         dec_input = tf.expand_dims(target[:, i], 1)\n","\n"," total_loss = (loss / int(target.shape[1]))\n"," trainable_variables = decoder.trainable_variables\n"," gradients = tape.gradient(loss,trainable_variables)\n"," optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n"," return loss, total_loss"],"id":"54f77c16","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"58963a15"},"source":["\n","EPOCHS = 10\n","for epoch in range(0, EPOCHS):\n","   start = time.time()\n","   total_loss = 0\n","\n","   for (batch, (img_tensor, target)) in enumerate(dataset):\n","       batch_loss, t_loss = train_step(img_tensor, target)\n","       total_loss += t_loss\n","\n","       if batch % 100 == 0:\n","           print ('Epoch {} Batch {} Loss {:.4f}'.format(\n","             epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","   # storing the epoch end loss value to plot later\n","   loss_plot.append(total_loss / num_steps)\n","\n","   print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n","                                        total_loss/num_steps))\n","\n","   print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"id":"58963a15","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3be64af7"},"source":[""],"id":"3be64af7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d48768a8"},"source":[""],"id":"d48768a8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"254ae5ad"},"source":["plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"],"id":"254ae5ad","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dabffca"},"source":["# evaluate using greedy"],"id":"9dabffca"},{"cell_type":"code","metadata":{"id":"999d9849"},"source":["def evaluate(vid):\n","   #attention_plot = np.zeros((max_length, attention_features_shape))\n","   #print(\"vid shape\",vid.shape)\n","\n","   hidden = decoder.reset_state(batch_size=1)\n","   temp_input = tf.expand_dims(vid, 0)\n","   max_length = 35\n","   #img_tensor_val = image_features_extract_model(temp_input)\n","   #img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3])\n","\n","   #features = encoder(img_tensor_val)\n","   #print(\"features shape\",vid.shape)\n","   features = temp_input\n","   #features = vid\n","   #print(\"feature shape\",features.shape)\n","   dec_input = tf.expand_dims([tokenizer.word_index['startseq']], 1)\n","   result = []\n","\n","   for i in range(max_length):\n","       #print(\"dec_input\" ,dec_input.shape)\n","       #print(\"features\",features.shape)\n","       #print(\"hidden\",hidden.shape)\n","       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","       #attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","       predicted_id = tf.argmax(predictions[0]).numpy()\n","       result.append(tokenizer.index_word[predicted_id])\n","       #print(\"current predicted word\",tokenizer.index_word[predicted_id])\n","\n","       if tokenizer.index_word[predicted_id] == 'endseq':\n","           return result\n","\n","       dec_input = tf.expand_dims([predicted_id], 0)\n","   #attention_plot = attention_plot[:len(result), :]\n","\n","   #return result, attention_plot\n","   return result"],"id":"999d9849","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ni0kt5zvQBao"},"source":["tokenizer.word_index['startseq']"],"id":"Ni0kt5zvQBao","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWa24-Ak55xm"},"source":["#!pip install jiwer"],"id":"CWa24-Ak55xm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqWp9zCt54GD"},"source":["import jiwer"],"id":"IqWp9zCt54GD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"erRWNK36emiB"},"source":["from nltk.translate.bleu_score import SmoothingFunction\n","smoothie = SmoothingFunction().method4"],"id":"erRWNK36emiB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veNsHGF1j7Dl"},"source":["actual,predicted = list(),list()\n","for i in range(len(data_dev)):\n","  image = data_dev[i]\n","  result = evaluate(image)\n","  real_caption = text_token_dev[i].split(' ')\n","  for j in real_caption:\n","    if((j=='startseq')or (j==\"endseq\")):\n","      real_caption.remove(j)\n","  first = ' '.join(j for j in real_caption)\n","  for j in result:\n","    if j==\"<unk>\":\n","      result.remove(j)\n","\n","  # for j in real_caption:\n","  #   if (j==\"<unk>\") or(j==\"endseq\") :\n","  #     real_caption.remove(j)\n","  #first = real_caption.split(' ', 1)[1]\n","  result_join = ' '.join(result)\n","  result_final = result_join.rsplit(' ', 1)[0]\n","  print(\"predicted sentence\",result_final)\n","  print(\"actual sentence\",first)\n","  actual.append(first)\n","  predicted.append(result_final)\n","  # print('BLEU-1: %f' % corpus_bleu([first],[result_final], weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","  # print('BLEU-2: %f' % corpus_bleu([first],[result_final], weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","  # print('BLEU-3: %f' % corpus_bleu([first],[result_final], weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","  # print('BLEU-4: %f' % corpus_bleu([first],[result_final], weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))\n","  print(\"word error rate is\",jiwer.wer(first,result_final))\n","    \n","\n","print('BLEU-1: %f' % corpus_bleu(actual,predicted, weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","print('BLEU-2: %f' % corpus_bleu(actual,predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","print('BLEU-3: %f' % corpus_bleu(actual,predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","print('BLEU-4: %f' % corpus_bleu(actual,predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))"],"id":"veNsHGF1j7Dl","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oIWc1NG4fEpD"},"source":["\n","image = data_dev[125]\n","\n","result = evaluate(image)\n","\n","real_caption = ' '.join([i for i in text_token_dev[125].split(' ')])\n","\n","\n","first = real_caption.split(' ', 1)[1]\n","\n","for i in result:\n","   if i==\"<unk>\":\n","       result.remove(i)\n","\n","for i in real_caption:\n","   if i==\"<unk>\":\n","       real_caption.remove(i)\n","\n","       \n","result_join = ' '.join(result)\n","result_final = result_join.rsplit(' ', 1)[0]\n","\n","print(\"final result \",result_final)\n","print(\"actaul \",first)\n","\n","\n","reference = first\n","candidate = result_final\n","print(\"ref length\",len(reference))\n","print(\"candidate length\",len(candidate))\n","\n","\n","print('BLEU-1: %f' % corpus_bleu([reference], [candidate], weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","print('BLEU-2: %f' % corpus_bleu([reference], [candidate], weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","print('BLEU-3: %f' % corpus_bleu([reference], [candidate], weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","print('BLEU-4: %f' % corpus_bleu([reference], [candidate], weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', result_final)\n"],"id":"oIWc1NG4fEpD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibiGsQVzLuA3"},"source":["# captions on the validation set\n","# rid = np.random.randint(0, len(img_name_val))\n","# image = '/content/gdrive/My Drive/FLICKR8K/Flicker8k_Dataset/2319175397_3e586cfaf8.jpg'\n","image = data_dev[125]\n","# real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","result = evaluate(image)\n","#print(\"result\",result)\n","real_caption = ' '.join([i for i in text_token_dev[125].split(' ')])\n","\n","#print(\"real caption\",real_caption)\n","# remove <start> and <end> from the real_caption\n","first = real_caption.split(' ', 1)[1]\n","#print(\"first ref\",first)\n","#real_caption = 'Two white dogs are playing in the snow'\n","\n","#remove \"<unk>\" in result\n","for i in result:\n","   if i==\"<unk>\":\n","       result.remove(i)\n","\n","for i in real_caption:\n","   if i==\"<unk>\":\n","       real_caption.remove(i)\n","\n","#remove <end> from result        \n","result_join = ' '.join(result)\n","result_final = result_join.rsplit(' ', 1)[0]\n","\n","print(\"final result \",result_final)\n","print(\"actaul \",first)\n","\n","# real_appn = []\n","# real_appn.append(real_caption.split())\n","reference = first\n","candidate = result_final\n","print(\"ref length\",len(reference))\n","print(\"candidate length\",len(candidate))\n","\n","#score = sentence_bleu(reference, candidate)\n","#print(f\"BELU score: {score*100}\")\n","print('BLEU-1: %f' % corpus_bleu([reference], [candidate], weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","print('BLEU-2: %f' % corpus_bleu([reference], [candidate], weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","print('BLEU-3: %f' % corpus_bleu([reference], [candidate], weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","print('BLEU-4: %f' % corpus_bleu([reference], [candidate], weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', result_final)\n","#plot_attention(image, result, attention_plot)"],"id":"ibiGsQVzLuA3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4d4b7990"},"source":["# def plot_attention(image, result, attention_plot):\n","#    temp_image = np.array(Image.open(image))\n","#    fig = plt.figure(figsize=(10, 10))\n","#    len_result = len(result)\n","#    for l in range(len_result):\n","#        temp_att = np.resize(attention_plot[l], (8, 8))\n","#        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n","#        ax.set_title(result[l])\n","#        img = ax.imshow(temp_image)\n","#        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","#    plt.tight_layout()\n","#    plt.show()"],"id":"4d4b7990","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"171f2b87"},"source":["# captions on the validation set\n","# rid = np.random.randint(0, 512)\n","# image = data_dev[0]\n","\n","for k in range(0,519,128):\n","  print(text_token_dev[k])\n","  # for j in text_token_dev[i].split(' '):\n","  #   print(j) \n","\n","  #real_caption = ' '.join([j for j in text_token_dev[k].split(' ')])\n","\n","  real_caption = []\n","  for j in text_token_dev[k:k+128]:\n","    real_caption.append(j)\n","  #print(\"real caption shape\",len(real_caption))\n","  \n","\n","\n","\n","  #print(\"data_dev.shape\",data_dev[k:k+128].shape)\n","\n","\n","\n","\n","\n","\n","\n","  #print(\"real_caption: \",real_caption)\n","  #print(\"data dev shape\",data_dev[k].shape)\n","  result = evaluate(data_dev[k:k+128,:])\n","  #print(\"result length\",len(result))\n","\n","  # remove <start> and <end> from the real_caption\n","  first = real_caption.split(' ', 1)[1]\n","  #real_caption = 'Two white dogs are playing in the snow'\n","\n","  #remove \"<unk>\" in result\n","\n","  for i in result:\n","    if i==\"<unk>\":\n","        result.remove(i)\n","\n","  for i in real_caption:\n","    if i==\"<unk>\":\n","        real_caption.remove(i)\n","\n","  #remove <end> from result        \n","  result_join = ' '.join(result)\n","  result_final = result_join.rsplit(' ', 1)[0]\n","\n","  real_appn = []\n","  real_appn.append(real_caption.split())\n","  reference = real_appn\n","  candidate = result\n","  break\n","  \n","\n","\n","score = sentence_bleu(reference, candidate)\n","print('BLEU-1: %f' % corpus_bleu(reference, candidate, weights=(1.0, 0, 0, 0)))\n","print('BLEU-2: %f' % corpus_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n","print('BLEU-3: %f' % corpus_bleu(reference, candidate, weights=(0.3, 0.3, 0.3, 0)))\n","print('BLEU-4: %f' % corpus_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))) \n","print(f\"BELU score: {score*100}\")\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', result_final)\n","#plot_attention(image, result, attention_plot)"],"id":"171f2b87","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_KmOQZ76f2E"},"source":["# test set"],"id":"c_KmOQZ76f2E"},{"cell_type":"code","metadata":{"id":"H5aGZCN26ifv"},"source":["actual,predicted = list(),list()\n","for i in range(len(data_test)):\n","  image = data_test[i]\n","  result = evaluate(image)\n","  real_caption = text_token_test[i].split(' ')\n","  for j in real_caption:\n","    if((j=='startseq')or (j==\"endseq\")):\n","      real_caption.remove(j)\n","  first = ' '.join(j for j in real_caption)\n","  for j in result:\n","    if j==\"<unk>\":\n","      result.remove(j)\n","\n","  # for j in real_caption:\n","  #   if (j==\"<unk>\") or(j==\"endseq\") :\n","  #     real_caption.remove(j)\n","  #first = real_caption.split(' ', 1)[1]\n","  result_join = ' '.join(result)\n","  result_final = result_join.rsplit(' ', 1)[0]\n","  print(\"predicted sentence\",result_final)\n","  print(\"actual sentence\",first)\n","  actual.append(first)\n","  predicted.append(result_final)\n","  # print('BLEU-1: %f' % corpus_bleu([first],[result_final], weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","  # print('BLEU-2: %f' % corpus_bleu([first],[result_final], weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","  # print('BLEU-3: %f' % corpus_bleu([first],[result_final], weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","  # print('BLEU-4: %f' % corpus_bleu([first],[result_final], weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))\n","  print(\"word error rate is\",jiwer.wer(first,result_final))\n","    \n","\n","print('BLEU-1: %f' % corpus_bleu(actual,predicted, weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n","print('BLEU-2: %f' % corpus_bleu(actual,predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n","print('BLEU-3: %f' % corpus_bleu(actual,predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n","print('BLEU-4: %f' % corpus_bleu(actual,predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))"],"id":"H5aGZCN26ifv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"17f7d002"},"source":["rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","start = time.time()\n","real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","\n","first = real_caption.split(' ', 1)[1]\n","real_caption = first.rsplit(' ', 1)[0]\n","\n","#remove \"<unk>\" in result\n","for i in result:\n","   if i==\"<unk>\":\n","       result.remove(i)\n","\n","#remove <end> from result        \n","result_join = ' '.join(result)\n","result_final = result_join.rsplit(' ', 1)[0]\n","\n","real_appn = []\n","real_appn.append(real_caption.split())\n","reference = real_appn\n","candidate = result_final\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', result_final)\n","\n","plot_attention(image, result, attention_plot)\n","print(f\"time took to Predict: {round(time.time()-start)} sec\")\n","\n","Image.open(img_name_val[rid])"],"id":"17f7d002","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"af25d74b"},"source":[""],"id":"af25d74b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61a793fd"},"source":[""],"id":"61a793fd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1c0b014"},"source":[""],"id":"e1c0b014","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ab8093a6"},"source":[""],"id":"ab8093a6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edf1af9b"},"source":[""],"id":"edf1af9b","execution_count":null,"outputs":[]}]}